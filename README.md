# LLM Evaluations Examples

## Reference-based metrics:
  - [notebooks/01_llm_evaluations_reference_based.ipynb](https://github.com/wesslen/llm-evaluations/blob/main/notebooks/01_llm_evaluations_reference_based.ipynb)
  - Overlap (BLEU, ROUGE, METEOR), Similarity (BERTscore), several others
  - [Huggingface's `evaluate`](https://huggingface.co/docs/evaluate/en/index) package for consistent implementation

## Llama Index + DeepEval: 
  - [notebooks/02_rag_deepeval_llama_index.ipynb](https://github.com/wesslen/llm-evaluations/blob/main/notebooks/02_rag_deepeval_llama_index.ipynb)
  - RAG Evaluation: Faithfulness via LLM-as-a-Judge (OpenAI) and Unit Testing
