{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRFp5q6BHYUNT/KAmTubeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5f838f30df944a3a5cc6b7c0dc03187": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e705a4e540e3482c9c84dd2badd0a2b1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mPrompt Alignment Metric\u001b[0m! \u001b[38;2;55;65;81m(using /models/NousResearch/Meta-Llama-3.1-8B-Instruc…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Prompt Alignment Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using /models/NousResearch/Meta-Llama-3.1-8B-Instruc…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e705a4e540e3482c9c84dd2badd0a2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948a641254ba4af3b4244cffdf8011be": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_79abf9107b924bfd9ed9eecb5a8c3a32",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using /models/NousResearch/Meta-Llama-3.1-8B-Instruc…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using /models/NousResearch/Meta-Llama-3.1-8B-Instruc…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "79abf9107b924bfd9ed9eecb5a8c3a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66706e4f349f4eac855c11f3b62b4f59": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_dff88f32505b4b0bbb9e732466f8fcf0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mToxicity Metric\u001b[0m! \u001b[38;2;55;65;81m(using /models/NousResearch/Meta-Llama-3.1-8B-Instruct, stric…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Toxicity Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using /models/NousResearch/Meta-Llama-3.1-8B-Instruct, stric…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "dff88f32505b4b0bbb9e732466f8fcf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-evaluations/blob/main/notebooks/deepeval_openai_compatible_endpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGTBRBvSNnJ7",
        "outputId": "be585f4d-7dc9-4fe3-c7d4-77fdbdb76119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.10.12 environment at /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m87 packages\u001b[0m \u001b[2min 508ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mpytest-asyncio\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.91 KiB/17.60 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mpytest-asyncio\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.91 KiB/17.60 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mpytest-asyncio\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.60 KiB/17.60 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 22ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytest-asyncio\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mresponses\u001b[0m\u001b[2m==0.25.3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system deepeval pytest pytest-asyncio responses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "LLM_API_KEY = userdata.get('DSBA_LLAMA3_KEY')\n",
        "LLM_ENDPOINT = userdata.get('LLAMA_BASE_URL')\n",
        "model_name = \"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "q3lU6ZrDNp4P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add at top of your script\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "RUIG4nWfqoH6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Dict, Any, Union\n",
        "import json\n",
        "import httpx\n",
        "import logging\n",
        "from pydantic import BaseModel, Field\n",
        "from deepeval.models import DeepEvalBaseLLM\n",
        "\n",
        "class LLMRequestError(Exception):\n",
        "    \"\"\"Custom exception for LLM request errors\"\"\"\n",
        "    def __init__(self, message: str, status_code: Optional[int] = None, response_text: Optional[str] = None):\n",
        "        self.status_code = status_code\n",
        "        self.response_text = response_text\n",
        "        super().__init__(f\"{message} - Status: {status_code}, Response: {response_text}\")\n",
        "\n",
        "class CustomOpenAILLM(DeepEvalBaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LLM class that implements OpenAI-compatible endpoints for DeepEval\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str,\n",
        "        model_name: str = \"gpt-3.5-turbo\",\n",
        "        base_url: Optional[str] = None,\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: int = 200,\n",
        "        verify_ssl: bool = True,\n",
        "        debug: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the custom LLM.\n",
        "\n",
        "        Args:\n",
        "            api_key (str): API key for authentication\n",
        "            model_name (str): Name of the model to use\n",
        "            base_url (Optional[str]): Base URL for the API endpoint\n",
        "            temperature (float): Sampling temperature\n",
        "            max_tokens (int): Maximum tokens to generate\n",
        "            verify_ssl (bool): Whether to verify SSL certificates\n",
        "            debug (bool): Enable debug logging\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "        self.base_url = (base_url or \"https://api.openai.com/v1\").rstrip('/')\n",
        "        self.temperature = min(max(temperature, 0.0), 1.0)  # Clamp between 0 and 1\n",
        "        self.max_tokens = max_tokens\n",
        "        self.verify_ssl = verify_ssl\n",
        "\n",
        "        if debug:\n",
        "            logging.basicConfig(level=logging.DEBUG)\n",
        "            self.logger = logging.getLogger(__name__)\n",
        "        else:\n",
        "            self.logger = logging.getLogger(__name__)\n",
        "            self.logger.setLevel(logging.WARNING)\n",
        "\n",
        "    def load_model(self) -> 'CustomOpenAILLM':\n",
        "        \"\"\"Required by DeepEval\"\"\"\n",
        "        return self\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        \"\"\"Required by DeepEval\"\"\"\n",
        "        return self.model_name\n",
        "\n",
        "    def _prepare_request_payload(self, prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Prepare the request payload\"\"\"\n",
        "        return {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens\n",
        "        }\n",
        "\n",
        "    def _prepare_headers(self) -> Dict[str, str]:\n",
        "        \"\"\"Prepare request headers\"\"\"\n",
        "        return {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "    def _handle_response(self, response: httpx.Response) -> str:\n",
        "        \"\"\"Handle the API response and extract the content\"\"\"\n",
        "        try:\n",
        "            response_data = response.json()\n",
        "            self.logger.debug(f\"Response data: {response_data}\")\n",
        "\n",
        "            if not response.is_success:\n",
        "                raise LLMRequestError(\n",
        "                    \"Request failed\",\n",
        "                    status_code=response.status_code,\n",
        "                    response_text=response.text\n",
        "                )\n",
        "\n",
        "            if \"choices\" not in response_data or not response_data[\"choices\"]:\n",
        "                raise LLMRequestError(\"No choices in response\",\n",
        "                                    response.status_code,\n",
        "                                    response.text)\n",
        "\n",
        "            if \"message\" not in response_data[\"choices\"][0]:\n",
        "                # Try alternative response formats\n",
        "                if \"text\" in response_data[\"choices\"][0]:\n",
        "                    return response_data[\"choices\"][0][\"text\"]\n",
        "                raise LLMRequestError(\"Unexpected response format\",\n",
        "                                    response.status_code,\n",
        "                                    response.text)\n",
        "\n",
        "            return response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            raise LLMRequestError(\"Invalid JSON response\",\n",
        "                                response.status_code,\n",
        "                                response.text)\n",
        "\n",
        "    def generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Union[str, BaseModel]:\n",
        "        \"\"\"Generate a response from the LLM\"\"\"\n",
        "        self.logger.debug(f\"Generating response for prompt: {prompt}\")\n",
        "\n",
        "        headers = self._prepare_headers()\n",
        "        payload = self._prepare_request_payload(prompt)\n",
        "\n",
        "        self.logger.debug(f\"Request URL: {self.base_url}/chat/completions\")\n",
        "        self.logger.debug(f\"Request Headers: {headers}\")\n",
        "        self.logger.debug(f\"Request Payload: {payload}\")\n",
        "\n",
        "        try:\n",
        "            with httpx.Client(verify=self.verify_ssl) as client:\n",
        "                response = client.post(\n",
        "                    f\"{self.base_url}/chat/completions\",\n",
        "                    headers=headers,\n",
        "                    json=payload,\n",
        "                    timeout=30.0\n",
        "                )\n",
        "\n",
        "                response_text = self._handle_response(response)\n",
        "\n",
        "                if schema:\n",
        "                    try:\n",
        "                        json_result = json.loads(response_text)\n",
        "                        return schema(**json_result)\n",
        "                    except json.JSONDecodeError:\n",
        "                        raise ValueError(\"Model output is not valid JSON\")\n",
        "                return response_text\n",
        "\n",
        "        except httpx.RequestError as e:\n",
        "            raise LLMRequestError(f\"Request failed: {str(e)}\")\n",
        "\n",
        "    async def a_generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Union[str, BaseModel]:\n",
        "        \"\"\"Generate a response from the LLM asynchronously\"\"\"\n",
        "        self.logger.debug(f\"Generating async response for prompt: {prompt}\")\n",
        "\n",
        "        headers = self._prepare_headers()\n",
        "        payload = self._prepare_request_payload(prompt)\n",
        "\n",
        "        try:\n",
        "            async with httpx.AsyncClient(verify=self.verify_ssl) as client:\n",
        "                response = await client.post(\n",
        "                    f\"{self.base_url}/chat/completions\",\n",
        "                    headers=headers,\n",
        "                    json=payload,\n",
        "                    timeout=30.0\n",
        "                )\n",
        "\n",
        "                response_text = self._handle_response(response)\n",
        "\n",
        "                if schema:\n",
        "                    try:\n",
        "                        json_result = json.loads(response_text)\n",
        "                        return schema(**json_result)\n",
        "                    except json.JSONDecodeError:\n",
        "                        raise ValueError(\"Model output is not valid JSON\")\n",
        "                return response_text\n",
        "\n",
        "        except httpx.RequestError as e:\n",
        "            raise LLMRequestError(f\"Async request failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "nk2Rb3Iv2kXw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For compatible endpoints without SSL verification\n",
        "llm_no_ssl = CustomOpenAILLM(\n",
        "    api_key=LLM_API_KEY,\n",
        "    model_name=model_name,\n",
        "    base_url=LLM_ENDPOINT,\n",
        "    verify_ssl=False\n",
        ")"
      ],
      "metadata": {
        "id": "Kfk7YzE5qE5Y"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_no_ssl.generate(\"What day is it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3p4DDad-xYWn",
        "outputId": "b1e2e8cf-aa2e-4b1d-eb0b-746687bb8826"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm not currently able to share the date.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import PromptAlignmentMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "metric = PromptAlignmentMetric(\n",
        "    prompt_instructions=[\"Reply in all uppercase\"],\n",
        "    model=llm_no_ssl,\n",
        "    include_reason=True\n",
        ")\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What if these shoes don't fit?\",\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=\"We offer a 30-day full refund at no extra cost.\"\n",
        ")\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "f5f838f30df944a3a5cc6b7c0dc03187",
            "e705a4e540e3482c9c84dd2badd0a2b1"
          ]
        },
        "id": "h93rXN7b1QAx",
        "outputId": "a51f6a37-f67c-4e90-f510-09b30f066484"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f838f30df944a3a5cc6b7c0dc03187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "The score is 0.00 because the LLM output does not contain the input in all uppercase letters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5,model=llm_no_ssl)\n",
        "test_case = LLMTestCase(\n",
        "  input=\"Who is the current president of the United States of America?\",\n",
        "  actual_output=\"Joe Biden\",\n",
        "  retrieval_context=[\"Joe Biden serves as the current president of America.\"]\n",
        ")\n",
        "\n",
        "answer_relevancy_metric.measure(test_case)\n",
        "print(answer_relevancy_metric.score)\n",
        "print(answer_relevancy_metric.reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "948a641254ba4af3b4244cffdf8011be",
            "79abf9107b924bfd9ed9eecb5a8c3a32"
          ]
        },
        "id": "T0rV1Gv3oxNt",
        "outputId": "6aa1a73f-1599-44bd-b558-77b43cf4e6a5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "948a641254ba4af3b4244cffdf8011be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "The score is 1.00 because the response directly answers the question without any irrelevant information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import ToxicityMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "\n",
        "metric = ToxicityMetric(model=llm_no_ssl)\n",
        "test_case = LLMTestCase(\n",
        "    input=\"How is Sarah as a person?\",\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=\"Sarah always meant well, but you couldn't help but sigh when she volunteered for a project.\"\n",
        ")\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "66706e4f349f4eac855c11f3b62b4f59",
            "dff88f32505b4b0bbb9e732466f8fcf0"
          ]
        },
        "id": "RtogLZ6a2nyv",
        "outputId": "03ee5efc-f96f-445d-c393-f443fd4e7934"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66706e4f349f4eac855c11f3b62b4f59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Model output is not valid JSON",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-c3d95904841e>\u001b[0m in \u001b[0;36ma_generate\u001b[0;34m(self, prompt, schema)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                         \u001b[0mjson_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mjson_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-fba9d0d4b121>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/toxicity/toxicity.py\u001b[0m in \u001b[0;36mmeasure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_or_create_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 loop.run_until_complete(\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/toxicity/toxicity.py\u001b[0m in \u001b[0;36ma_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     97\u001b[0m             )\n\u001b[1;32m     98\u001b[0m             self.verdicts: List[ToxicityVerdict] = (\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a_generate_verdicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/toxicity/toxicity.py\u001b[0m in \u001b[0;36m_a_generate_verdicts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 res: Verdicts = await self.model.a_generate(\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVerdicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 )\n",
            "\u001b[0;32m<ipython-input-57-c3d95904841e>\u001b[0m in \u001b[0;36ma_generate\u001b[0;34m(self, prompt, schema)\u001b[0m\n\u001b[1;32m    166\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mjson_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model output is not valid JSON\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model output is not valid JSON"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_endpoint():\n",
        "    llm = CustomOpenAILLM(\n",
        "        api_key=LLM_API_KEY,\n",
        "        model_name=model_name,\n",
        "        base_url=LLM_ENDPOINT,\n",
        "        verify_ssl=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm.generate(\"This is a test prompt\")\n",
        "        print(\"Success:\", response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        if hasattr(e, 'response'):\n",
        "            print(f\"Response status: {e.response.status_code}\")\n",
        "            print(f\"Response body: {e.response.text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_endpoint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX0rKNx5wQ7M",
        "outputId": "674365ce-d6aa-4da8-8c54-b70fc88a8a44"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: A blank slate! I'm ready to respond. How would you like this test to proceed?\n"
          ]
        }
      ]
    }
  ]
}